# ChatGLM Efficient Tuning

[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/ChatGLM-Efficient-Tuning?style=social)](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/stargazers)
[![GitHub Code License](https://img.shields.io/github/license/hiyouga/ChatGLM-Efficient-Tuning)](LICENSE)
[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/ChatGLM-Efficient-Tuning)](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/commits/main)
[![PyPI](https://img.shields.io/pypi/v/glmtuner)](https://pypi.org/project/glmtuner/)
[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/pulls)

åŸºäº ğŸ¤—[PEFT](https://github.com/huggingface/peft) çš„é«˜æ•ˆ ğŸ¤–[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) å¾®è°ƒã€‚

ğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„[å¾®ä¿¡ç¾¤](assets/wechat.jpg)ã€‚

\[ [English](README.md) | ä¸­æ–‡ \]

å¦‚æœæœ‰ä»»ä½•ç–‘é—®ï¼Œè¯·é˜…è¯»æˆ‘ä»¬çš„ [æ–‡æ¡£ ğŸ“„](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/wiki)ã€‚

## å…¬å‘Š

è¯¥é¡¹ç›®ä»Šå**å°†ä¸å†ç»´æŠ¤**ã€‚è¯·å…³æ³¨ **[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)** å¤§æ¨¡å‹å¾®è°ƒé¡¹ç›®ï¼ˆåŒ…æ‹¬ ChatGLM2-6B æ¨¡å‹ï¼‰ã€‚

## æ›´æ–°æ—¥å¿—

[23/07/15] æˆ‘ä»¬å¼€å‘äº†æ”¯æŒè®­ç»ƒå’Œæµ‹è¯•çš„æµè§ˆå™¨ä¸€é”®å¾®è°ƒç•Œé¢ã€‚è¯·å°è¯•ä½¿ç”¨ `train_web.py` åœ¨æ‚¨çš„æµè§ˆå™¨ä¸­å¾®è°ƒ ChatGLM-6B æ¨¡å‹ã€‚æ„Ÿè°¢ [@KanadeSiina](https://github.com/KanadeSiina) å’Œ [@codemayq](https://github.com/codemayq) åœ¨è¯¥åŠŸèƒ½å¼€å‘ä¸­ä»˜å‡ºçš„åŠªåŠ›ã€‚

[23/07/09] æˆ‘ä»¬å¼€æºäº† [FastEdit](https://github.com/hiyouga/FastEdit)âš¡ğŸ©¹ï¼Œä¸€ä¸ªç®€å•æ˜“ç”¨çš„ã€èƒ½è¿…é€Ÿç¼–è¾‘å¤§æ¨¡å‹äº‹å®è®°å¿†çš„å·¥å…·åŒ…ã€‚å¦‚æœæ‚¨æ„Ÿå…´è¶£è¯·å…³æ³¨æˆ‘ä»¬çš„ [FastEdit](https://github.com/hiyouga/FastEdit) é¡¹ç›®ã€‚

[23/06/25] æˆ‘ä»¬å¯¹é½äº†[ç¤ºä¾‹ API](src/api_demo.py) ä¸ [OpenAI API](https://platform.openai.com/docs/api-reference/chat) çš„æ ¼å¼ï¼Œæ‚¨å¯ä»¥å°†å¾®è°ƒæ¨¡å‹æ¥å…¥ä»»æ„åŸºäº ChatGPT çš„åº”ç”¨ä¸­ã€‚

[23/06/25] ç°åœ¨æˆ‘ä»¬å®ç°äº† [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) æ¨¡å‹çš„å¾®è°ƒã€‚

[23/06/05] ç°åœ¨æˆ‘ä»¬å®ç°äº† 4 æ¯”ç‰¹çš„ LoRA è®­ç»ƒï¼ˆä¹Ÿç§° [QLoRA](https://github.com/artidoro/qlora)ï¼‰ã€‚è¯·å°è¯•ä½¿ç”¨ `--quantization_bit 4` å‚æ•°è¿›è¡Œ 4 æ¯”ç‰¹é‡åŒ–å¾®è°ƒã€‚ï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰

[23/06/01] æˆ‘ä»¬å¼€æºäº†æ”¯æŒ LLaMA å’Œ BLOOM ç³»åˆ—æ¨¡å‹çš„é«˜æ•ˆå¾®è°ƒæ¡†æ¶ï¼Œå¦‚æœæ‚¨æ„Ÿå…´è¶£è¯·å…³æ³¨æˆ‘ä»¬çš„ [LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) é¡¹ç›®ã€‚

[23/06/01] æˆ‘ä»¬æ–°å¢äº†ä¸€ä¸ªä½¿ç”¨ç›‘ç£å¾®è°ƒå’Œ RLHF è®­ç»ƒåŒ»ç–—é—®ç­”æ¨¡å‹çš„ä¾‹å­ï¼Œè¯·ç§»æ­¥ [covid_doctor.md](examples/covid_doctor.md) æŸ¥é˜…ã€‚

[23/05/19] ç°åœ¨æˆ‘ä»¬æ”¯æŒäº†åœ¨æ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨éªŒè¯é›†è¯„ä¼°æ€§èƒ½ã€‚è¯·å°è¯•ä½¿ç”¨ `--dev_ratio` å‚æ•°æŒ‡å®šéªŒè¯é›†å¤§å°ã€‚

[23/04/29] ç°åœ¨æˆ‘ä»¬å®ç°äº† **RLHFï¼ˆåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼‰** è®­ç»ƒï¼æˆ‘ä»¬æä¾›äº†å‡ ä¸ªè¿è¡Œ RLHF çš„ä¾‹å­ï¼Œå…·ä½“å†…å®¹è¯·ç§»æ­¥ `examples` æ–‡ä»¶å¤¹ã€‚

[23/04/25] æˆ‘ä»¬æ–°å¢äº†ä¸€ä¸ªä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†åˆ†å¸ƒå¼è®­ç»ƒçš„ä¾‹å­ï¼Œè¯·ç§»æ­¥ [ads_generation.md](examples/ads_generation.md) æŸ¥é˜…ã€‚

[23/04/20] æˆ‘ä»¬çš„é¡¹ç›®åœ¨ 12 å¤©å†…è·å¾—äº† 100 ä¸ª Starï¼ç¥è´ºï¼

[23/04/20] æˆ‘ä»¬æ–°å¢äº†ä¸€ä¸ªä¿®æ”¹æ¨¡å‹è‡ªæˆ‘è®¤çŸ¥çš„ä¾‹å­ï¼Œè¯·ç§»æ­¥ [alter_self_cognition.md](examples/alter_self_cognition.md) æŸ¥é˜…ã€‚

[23/04/19] ç°åœ¨æˆ‘ä»¬å®ç°äº†**æ¨¡å‹èåˆ**ï¼è¯·å°è¯•ä½¿ç”¨ `--checkpoint_dir checkpoint1,checkpoint2` å‚æ•°è®­ç»ƒèåˆ LoRA æƒé‡åçš„æ¨¡å‹ã€‚

[23/04/18] ç°åœ¨å¯ä»¥å¾®è°ƒ**é‡åŒ–æ¨¡å‹**äº†ï¼è¯·å°è¯•ä½¿ç”¨ `quantization_bit` å‚æ•°è¿›è¡Œ 4 æ¯”ç‰¹æˆ– 8 æ¯”ç‰¹é‡åŒ–å¾®è°ƒã€‚

[23/04/12] ç°åœ¨æˆ‘ä»¬åŠ å…¥äº†**æ–­ç‚¹è®­ç»ƒæ”¯æŒ**ï¼è¯·å°è¯•ç»™å®š `--checkpoint_dir` å‚æ•°åŠ è½½æŒ‡å®šçš„æ¨¡å‹æ–­ç‚¹ã€‚

[23/04/11] ç°åœ¨æˆ‘ä»¬å®ç°äº†**æ•°æ®é›†ç»„åˆè®­ç»ƒ**ï¼è¯·å°è¯•ä½¿ç”¨ `--dataset dataset1,dataset2` å‚æ•°è¿›è¡Œç»„åˆè®­ç»ƒã€‚

## æ•°æ®é›†

- SFT è®­ç»ƒï¼š
  - [Stanford Alpaca (en)](https://github.com/tatsu-lab/stanford_alpaca)
  - [Stanford Alpaca (zh)](https://github.com/ymcui/Chinese-LLaMA-Alpaca)
  - [GPT-4 Generated Data (en&zh)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)
  - [Open Assistant (multilingual)](https://huggingface.co/datasets/OpenAssistant/oasst1)
  - [Self-cognition (zh)](data/self_cognition.json)
  - [ShareGPT (zh)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT/tree/main/Chinese-instruction-collection)
  - [RefGPT (zh)](https://github.com/sufengniu/RefGPT)
  - [Guanaco Dataset (multilingual)](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)
  - [BELLE 2M (zh)](https://huggingface.co/datasets/BelleGroup/train_2M_CN)
  - [BELLE 1M (zh)](https://huggingface.co/datasets/BelleGroup/train_1M_CN)
  - [BELLE 0.5M (zh)](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)
  - [BELLE Dialogue 0.4M (zh)](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M)
  - [BELLE School Math 0.25M (zh)](https://huggingface.co/datasets/BelleGroup/school_math_0.25M)
  - [BELLE Multiturn Chat 0.8M (zh)](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M)
  - [Firefly 1.1M (zh)](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)
  - [LIMA (en)](https://huggingface.co/datasets/GAIR/lima)
  - [CodeAlpaca 20k (en)](https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k)
  - [Alpaca CoT (multilingual)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)
  - [Web QA (zh)](https://huggingface.co/datasets/suolyer/webqa)
  - [UltraChat (en)](https://github.com/thunlp/UltraChat)
  - [WebNovel (zh)](https://huggingface.co/datasets/zxbsmk/webnovel_cn)
- å¥–åŠ±æ¨¡å‹è®­ç»ƒï¼š
  - [HH-RLHF (en)](https://huggingface.co/datasets/Anthropic/hh-rlhf)
  - [Open Assistant (multilingual)](https://huggingface.co/datasets/OpenAssistant/oasst1)
  - [GPT-4 Generated Data (en&zh)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)

ä½¿ç”¨æ–¹æ³•è¯·å‚è€ƒ [data/README.md](data/README_zh.md) æ–‡ä»¶ã€‚

éƒ¨åˆ†æ•°æ®é›†çš„ä½¿ç”¨éœ€è¦ç¡®è®¤ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨ä¸‹è¿°å‘½ä»¤ç™»å½•æ‚¨çš„ Hugging Face è´¦æˆ·ã€‚

```bash
pip install --upgrade huggingface_hub
huggingface-cli login
```

## å¾®è°ƒæ–¹æ³•

ç›®å‰æˆ‘ä»¬å®ç°äº†é’ˆå¯¹ä»¥ä¸‹é«˜æ•ˆå¾®è°ƒæ–¹æ³•çš„æ”¯æŒï¼š

- [LoRA](https://arxiv.org/abs/2106.09685)
  - ä»…å¾®è°ƒä½ç§©é€‚åº”å™¨ã€‚
- [P-Tuning V2](https://github.com/THUDM/P-tuning-v2)
  - ä»…å¾®è°ƒå‰ç¼€ç¼–ç å™¨ã€‚
- [Freeze Tuning](https://arxiv.org/abs/2012.14913)
  - ä»…å¾®è°ƒåå‡ å±‚çš„å…¨è¿æ¥å±‚ã€‚
- å…¨é‡å¾®è°ƒ
  - å¾®è°ƒæ¨¡å‹æ‰€æœ‰å‚æ•°ã€‚

## è½¯ä»¶ä¾èµ–

- Python 3.8+, PyTorch 1.13.1
- ğŸ¤—Transformers, Datasets, Accelerate, PEFT, TRL
- protobuf, cpm-kernels, sentencepiece
- jieba, rouge-chinese, nltkï¼ˆç”¨äºè¯„ä¼°ï¼‰
- gradio, matplotlibï¼ˆç”¨äºç½‘é¡µç«¯äº¤äº’ï¼‰
- uvicorn, fastapi, sse-starletteï¼ˆç”¨äº APIï¼‰

ä»¥åŠ **å¼ºè€Œæœ‰åŠ›çš„ GPU**ï¼

## å¦‚ä½•ä½¿ç”¨

### æ•°æ®å‡†å¤‡ï¼ˆå¯è·³è¿‡ï¼‰

å…³äºæ•°æ®é›†æ–‡ä»¶çš„æ ¼å¼ï¼Œè¯·å‚è€ƒ `data/example_dataset` æ–‡ä»¶å¤¹çš„å†…å®¹ã€‚æ„å»ºè‡ªå®šä¹‰æ•°æ®é›†æ—¶ï¼Œæ—¢å¯ä»¥ä½¿ç”¨å•ä¸ª `.json` æ–‡ä»¶ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ä¸€ä¸ª[æ•°æ®åŠ è½½è„šæœ¬](https://huggingface.co/docs/datasets/dataset_script)å’Œå¤šä¸ªæ–‡ä»¶ã€‚

æ³¨æ„ï¼šä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†æ—¶ï¼Œè¯·æ›´æ–° `data/dataset_info.json` æ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶çš„æ ¼å¼è¯·å‚è€ƒ `data/README.md`ã€‚

### ç¯å¢ƒæ­å»ºï¼ˆå¯è·³è¿‡ï¼‰

```bash
git lfs install
git clone https://github.com/hiyouga/ChatGLM-Efficient-Tuning.git
conda create -n chatglm_etuning python=3.10
conda activate chatglm_etuning
cd ChatGLM-Efficient-Tuning
pip install -r requirements.txt
```

å¦‚æœè¦åœ¨ Windows å¹³å°ä¸Šå¼€å¯é‡åŒ– LoRAï¼ˆQLoRAï¼‰ï¼Œéœ€è¦å®‰è£…é¢„ç¼–è¯‘çš„ `bitsandbytes` åº“, æ”¯æŒ CUDA 11.1 åˆ° 12.1.

```bash
pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.39.1-py3-none-win_amd64.whl
```

### æµè§ˆå™¨ä¸€é”®å¾®è°ƒ/æµ‹è¯•

```bash
CUDA_VISIBLE_DEVICES=0 python src/train_web.py
```

ç›®å‰ç½‘é¡µ UI ä»…æ”¯æŒ**å•å¡è®­ç»ƒ**ã€‚

### å• GPU å¾®è°ƒè®­ç»ƒ

```bash
CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \
    --stage sft \
    --model_name_or_path path_to_your_chatglm_model \
    --do_train \
    --dataset alpaca_gpt4_zh \
    --finetuning_type lora \
    --output_dir path_to_sft_checkpoint \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate 5e-5 \
    --num_train_epochs 3.0 \
    --plot_loss \
    --fp16
```

å…³äºå‚æ•°ä¿¡æ¯ï¼Œè¯·æŸ¥é˜…æˆ‘ä»¬çš„[ç»´åŸº](https://github.com/hiyouga/ChatGLM-Efficient-Tuning/wiki)ã€‚

### å¤š GPU åˆ†å¸ƒå¼å¾®è°ƒ

```bash
accelerate config # é¦–å…ˆé…ç½®åˆ†å¸ƒå¼ç¯å¢ƒ
accelerate launch src/train_bash.py # å‚æ•°åŒä¸Š
```

### å¥–åŠ±æ¨¡å‹è®­ç»ƒ

```bash
CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \
    --stage rm \
    --model_name_or_path path_to_your_chatglm_model \
    --do_train \
    --dataset comparison_gpt4_zh \
    --finetuning_type lora \
    --resume_lora_training False \
    --checkpoint_dir path_to_sft_checkpoint \
    --output_dir path_to_rm_checkpoint \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate 1e-5 \
    --num_train_epochs 1.0 \
    --plot_loss \
    --fp16
```

### RLHF è®­ç»ƒ

```bash
CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \
    --stage ppo \
    --model_name_or_path path_to_your_chatglm_model \
    --do_train \
    --dataset alpaca_gpt4_zh \
    --finetuning_type lora \
    --resume_lora_training False \
    --checkpoint_dir path_to_sft_checkpoint \
    --reward_model path_to_rm_checkpoint \
    --output_dir path_to_ppo_checkpoint \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 4 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate 1e-5 \
    --num_train_epochs 1.0 \
    --plot_loss
```

### æŒ‡æ ‡è¯„ä¼°ï¼ˆBLEUåˆ†æ•°å’Œæ±‰è¯­ROUGEåˆ†æ•°ï¼‰

```bash
CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \
    --stage sft \
    --model_name_or_path path_to_your_chatglm_model \
    --do_eval \
    --dataset alpaca_gpt4_zh \
    --finetuning_type lora \
    --checkpoint_dir path_to_checkpoint \
    --output_dir path_to_eval_result \
    --per_device_eval_batch_size 8 \
    --max_samples 50 \
    --predict_with_generate
```

### æ¨¡å‹é¢„æµ‹

```bash
CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \
    --stage sft \
    --model_name_or_path path_to_your_chatglm_model \
    --do_predict \
    --dataset alpaca_gpt4_zh \
    --finetuning_type lora \
    --checkpoint_dir path_to_checkpoint \
    --output_dir path_to_predict_result \
    --per_device_eval_batch_size 8 \
    --max_samples 50 \
    --predict_with_generate
```

æ³¨ï¼šå¦‚æœéœ€è¦é¢„æµ‹çš„æ ·æœ¬æ²¡æœ‰æ ‡ç­¾ï¼Œè¯·é¦–å…ˆåœ¨ `response` åˆ—ä¸­å¡«å…¥ä¸€äº›å ä½ç¬¦ï¼Œä»¥å…æ ·æœ¬åœ¨é¢„å¤„ç†é˜¶æ®µè¢«ä¸¢å¼ƒã€‚

### API æœåŠ¡

```bash
python src/api_demo.py \
    --model_name_or_path path_to_your_chatglm_model \
    --finetuning_type lora \
    --checkpoint_dir path_to_checkpoint
```

å…³äº API æ–‡æ¡£è¯·è§ `http://localhost:8000/docs`ã€‚

### å‘½ä»¤è¡Œæµ‹è¯•

```bash
python src/cli_demo.py \
    --model_name_or_path path_to_your_chatglm_model \
    --finetuning_type lora \
    --checkpoint_dir path_to_checkpoint
```

### æµè§ˆå™¨æµ‹è¯•

```bash
python src/web_demo.py \
    --model_name_or_path path_to_your_chatglm_model \
    --finetuning_type lora \
    --checkpoint_dir path_to_checkpoint
```

### å¯¼å‡ºå¾®è°ƒæ¨¡å‹

```bash
python src/export_model.py \
    --model_name_or_path path_to_your_chatglm_model \
    --finetuning_type lora \
    --checkpoint_dir path_to_checkpoint \
    --output_dir path_to_export
```

### ç¡¬ä»¶éœ€æ±‚

|     å¾®è°ƒæ–¹æ³•     |  æ‰¹å¤„ç†å¤§å°  | æ¨¡å¼ | GPUæ˜¾å­˜ | é€Ÿåº¦ |
| ---------------- | ---------- | ---- | ------ | ----- |
| LoRA (r=8)       |     16     | FP16 |  28GB  | 8ex/s |
| LoRA (r=8)       |     8      | FP16 |  24GB  | 8ex/s |
| LoRA (r=8)       |     4      | FP16 |  20GB  | 8ex/s |
| LoRA (r=8)       |     4      | INT8 |  10GB  | 8ex/s |
| LoRA (r=8)       |     4      | INT4 |   8GB  | 8ex/s |
| P-Tuning (p=16)  |     4      | FP16 |  20GB  | 8ex/s |
| P-Tuning (p=16)  |     4      | INT8 |  16GB  | 8ex/s |
| P-Tuning (p=16)  |     4      | INT4 |  12GB  | 8ex/s |
| Freeze (l=3)     |     4      | FP16 |  24GB  | 8ex/s |

| å¥–åŠ±æ¨¡å‹è®­ç»ƒæ–¹æ³• |  æ‰¹å¤„ç†å¤§å°  | æ¨¡å¼ | GPUæ˜¾å­˜ | é€Ÿåº¦ |
| --------------- | ----------  | ---- | ------ | ---- |
| LoRA (r=8) + rm |      4      | FP16 |  22GB  | -    |
| LoRA (r=8) + rm |      1      | INT8 |  11GB  | -    |

|   RLHF è®­ç»ƒæ–¹æ³•   |  æ‰¹å¤„ç†å¤§å°  | æ¨¡å¼ | GPUæ˜¾å­˜ | é€Ÿåº¦ |
| ---------------- | ----------  | ---- | ------ | ---- |
| LoRA (r=8) + ppo |      4      | FP16 |  23GB  | -    |
| LoRA (r=8) + ppo |      1      | INT8 |  12GB  | -    |

> æ³¨ï¼š`r` ä¸ºLoRA ç»´æ•°å¤§å°ï¼Œ`p` ä¸ºå‰ç¼€è¯è¡¨å¤§å°ï¼Œ`l` ä¸ºå¾®è°ƒå±‚æ•°ï¼Œ`ex/s` ä¸ºæ¯ç§’è®­ç»ƒçš„æ ·æœ¬æ•°ã€‚`gradient_accumulation_steps` å‚æ•°è®¾ç½®ä¸º `1`ã€‚ä¸Šè¿°ç»“æœå‡æ¥è‡ªäºå•ä¸ª Tesla V100 GPUï¼Œä»…ä¾›å‚è€ƒã€‚

## å¾®è°ƒ ChatGLM çš„ä¾‹å­

### è®­ç»ƒç»“æœ

æˆ‘ä»¬ä½¿ç”¨æ•´ä¸ª `alpaca_gpt4_zh` æ•°æ®é›†å¾®è°ƒ ChatGLM æ¨¡å‹ï¼Œä½¿ç”¨ç§©ä¸º 8 çš„ LoRA æ–¹æ³•ï¼Œä½¿ç”¨é»˜è®¤è¶…å‚æ•°è¿›è¡Œå•è½®è®­ç»ƒã€‚ä¸‹å›¾ä¸ºè®­ç»ƒæŸå¤±å˜åŒ–æ›²çº¿ã€‚

![è®­ç»ƒæŸå¤±](assets/trainer_state.jpg)

### è¯„ä¼°ç»“æœ

æˆ‘ä»¬é€‰æ‹© `alpaca_gpt4_zh` æ•°æ®é›†ä¸­çš„å‰ä¸€ç™¾æ¡æ•°æ®æ¥è¯„ä¼°å¾®è°ƒåçš„ ChatGLM æ¨¡å‹ï¼Œå¹¶è®¡ç®— BLEU å’Œä¸­æ–‡ ROUGE åˆ†æ•°ã€‚ä¸‹è¡¨ä¸ºè¯„ä¼°ç»“æœã€‚

|   åˆ†æ•°  |  åŸç‰ˆæ¨¡å‹ | FZ (l=2) | PT (p=16) | LoRA (r=8) |
| ------- | -------- | ----- | ----- | ----------------- |
| BLEU-4  |  15.75   | 16.85 | 16.06 | 17.01 (**+1.26**) |
| Rouge-1 |  34.51   | 36.62 | 34.80 | 36.77 (**+2.26**) |
| Rouge-2 |  15.11   | 17.04 | 15.32 | 16.83 (**+1.72**) |
| Rouge-l |  26.18   | 28.17 | 26.35 | 28.86 (**+2.68**) |
| è®­ç»ƒå‚æ•° |  /       | 4.35% | 0.06% | 0.06%             |

> FZï¼šFreeze å¾®è°ƒï¼ŒPTï¼šP-Tuning V2 å¾®è°ƒï¼ˆä¸ºäº†ä¸ LoRA å…¬å¹³æ¯”è¾ƒï¼Œæˆ‘ä»¬ä½¿ç”¨äº† `pre_seq_len=16`ï¼‰ï¼Œè®­ç»ƒå‚æ•°ï¼šå¯è®­ç»ƒå‚æ•°å å…¨éƒ¨å‚æ•°çš„ç™¾åˆ†æ¯”ã€‚

## å‹æƒ…é“¾æ¥

- [SupritYoung/RLHF-Label-Tool](https://github.com/SupritYoung/RLHF-Label-Tool/tree/master)ï¼šä¸€ä¸ªç»™å¤§æ¨¡å‹ç”Ÿæˆç»“æœè¿›è¡Œæ’åºï¼Œä»è€Œè·å¾—ç”¨äº RLHF è®­ç»ƒçš„æ ‡æ³¨æ•°æ®çš„å¹³å°ã€‚

## å’Œç°æœ‰ç±»ä¼¼é¡¹ç›®çš„æ¯”è¾ƒ

- [THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B/tree/main/ptuning)
  - ChatGLM åŸºäº [P-Tuning v2](https://github.com/THUDM/P-tuning-v2) å¾®è°ƒçš„å®˜æ–¹å®ç°ï¼Œä½¿ç”¨äº† [ADGEN](https://aclanthology.org/D19-1321.pdf) æ•°æ®é›†ã€‚
  - æœ¬ä»“åº“çš„ä»£ç å®ç°ç»å¤§éƒ¨åˆ†å‚è€ƒè¯¥é¡¹ç›®ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å®ç°äº† [LoRA](https://arxiv.org/abs/2106.09685) å¾®è°ƒæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬**åŠ¨æ€åœ°**å°†æ¯ä¸ªæ‰¹å¤„ç†æ•°æ®ä¸­çš„åºåˆ—è¿›è¡Œå¡«å……ï¼Œè€Œéå°†å…¶å¡«å……åˆ°æ¨¡å‹çš„æœ€å¤§é•¿åº¦ï¼Œæ­¤æ”¹è¿›å¯ä»¥åŠ é€Ÿæ¨¡å‹è®­ç»ƒã€‚
- [mymusise/ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning)
  - ChatGLM åŸºäº [LoRA](https://arxiv.org/abs/2106.09685) å¾®è°ƒçš„éå®˜æ–¹å®ç°ï¼Œä½¿ç”¨äº† [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) æ•°æ®é›†ã€‚
  - æˆ‘ä»¬å€Ÿé‰´äº†è¯¥é¡¹ç›®çš„ä¸€äº›æƒ³æ³•ã€‚æˆ‘ä»¬çš„è®­ç»ƒè„šæœ¬å°†æ•°æ®é¢„å¤„ç†éƒ¨åˆ†**é›†æˆ**è‡³è®­ç»ƒè„šæœ¬ä¸­ï¼Œä»¥é¿å…äº‹å…ˆç”Ÿæˆé¢„å¤„ç†åçš„æ•°æ®ã€‚
- [ssbuild/chatglm_finetuning](https://github.com/ssbuild/chatglm_finetuning)
  - ChatGLM åŸºäºå¤šç§å¾®è°ƒæ–¹æ³•çš„éå®˜æ–¹å®ç°ï¼Œä½¿ç”¨äº† [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) æ•°æ®é›†ã€‚
  - æˆ‘ä»¬çš„è®­ç»ƒè„šæœ¬**å…¨éƒ¨**åŸºäº [Hugging Face transformers](https://github.com/huggingface/transformers) æ¡†æ¶å®ç°ï¼Œä¸ä¾èµ–äºé¢å¤–çš„ [deep_training](https://github.com/ssbuild/deep_training) æ¡†æ¶ã€‚
- [lich99/ChatGLM-finetune-LoRA](https://github.com/lich99/ChatGLM-finetune-LoRA)
  - ChatGLM åŸºäº [LoRA](https://arxiv.org/abs/2106.09685) å¾®è°ƒçš„éå®˜æ–¹å®ç°ï¼Œä½¿ç”¨äº† [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) æ•°æ®é›†ã€‚
  - æˆ‘ä»¬åˆ©ç”¨ [Hugging Face PEFT](https://github.com/huggingface/peft) æ¡†æ¶æ¥å¼•å…¥æœ€å…ˆè¿›çš„å¾®è°ƒæ–¹æ³•ã€‚
- [liucongg/ChatGLM-Finetuning](https://github.com/liucongg/ChatGLM-Finetuning)
  - ChatGLM åŸºäºå‚æ•°å†»ç»“ã€LoRA å’Œ P-Tuning å¾®è°ƒçš„éå®˜æ–¹å®ç°ï¼Œä½¿ç”¨äº†æ±½è½¦å·¥ä¸šæ•°æ®é›†ã€‚
  - æˆ‘ä»¬æ—¨åœ¨å¼•å…¥æ›´å¤šæŒ‡ä»¤éµå¾ªæ•°æ®é›†ç”¨äºå¾®è°ƒ ChatGLM æ¨¡å‹ã€‚
- [yanqiangmiffy/InstructGLM](https://github.com/yanqiangmiffy/InstructGLM)
  - ChatGLM å¾®è°ƒçš„éå®˜æ–¹å®ç°ï¼Œæ—¨åœ¨æ¢ç´¢ ChatGLM åœ¨æŒ‡ä»¤éµå¾ªæ•°æ®é›†ä¸Šçš„æ½œåŠ›ã€‚
  - æˆ‘ä»¬å°†æ•°æ®é¢„å¤„ç†éƒ¨åˆ†é›†æˆåˆ°è®­ç»ƒè„šæœ¬ä¸­ã€‚

## TODO

- [ ] åˆ©ç”¨ [LangChain](https://github.com/hwchase17/langchain) å®ç°èƒ½å¤Ÿåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†çš„åŸºäº ChatGLM å¾®è°ƒæ¨¡å‹åº”ç”¨çš„è½»æ¾æ„å»ºã€‚
- [ ] å®ç°å¯¹é½ç®—æ³•ä½¿æ¨¡å‹å¯¹é½äººç±»æ„å›¾ã€‚
  - [x] [RLHF](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat)
  - [ ] [RRHF](https://github.com/GanjinZero/RRHF)
  - [ ] [RAFT](https://github.com/OptimalScale/LMFlow)
- [ ] åŠ å…¥æ›´å¤š[ä¸­æ–‡æ•°æ®é›†](https://github.com/brightmart/nlp_chinese_corpus)ã€‚
  - [x] [BELLE](https://github.com/LianjiaTech/BELLE)
  - [ ] [pCLUE](https://github.com/CLUEbenchmark/pCLUE)
  - [ ] [CLUECorpus](https://github.com/CLUEbenchmark/CLUECorpus2020)
  - [x] [GuanacoDataset](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)
  - [x] [FireflyDataset](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)
- [ ] åŠ å…¥åŸºäº [ChatGPT](https://openai.com/blog/chatgpt) å’Œ [GPT-4](https://openai.com/research/gpt-4) äº§ç”Ÿçš„æ•°æ®é›†ã€‚
  - [ ] [Baize](https://github.com/project-baize/baize-chatbot)
  - [x] [GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)
- [x] å®ç°å‚æ•°å†»ç»“å’Œ P-Tuning å¾®è°ƒæ–¹æ³•ã€‚
- [x] æ”¯æŒå¤šGPUè®­ç»ƒã€‚
- [x] åŠ å…¥æ¨¡å‹è¯„ä¼°è„šæœ¬ã€‚
- [x] æ–­ç‚¹åŠ è½½ã€‚
- [x] é‡åŒ–å¾®è°ƒã€‚
- [x] æ’°å†™åŸºäºè¯¥æ¡†æ¶çš„ ChatGLM æ¨¡å‹å¾®è°ƒæŒ‡å—æ‰‹å†Œã€‚
- [ ] ç»“åˆæ¨¡å‹ç¼–è¾‘æŠ€æœ¯ã€‚ï¼ˆä¾‹å¦‚ï¼š[MEND](https://arxiv.org/abs/2110.11309)ï¼‰
- [x] åŠ å…¥ [OpenAssistant å¯¹è¯æ•°æ®é›†](https://huggingface.co/datasets/OpenAssistant/oasst1)ç”¨äºç›‘ç£å¾®è°ƒå’Œæ„å›¾å¯¹é½ã€‚
- [ ] åŠ å…¥é«˜è´¨é‡ä¸­æ–‡å¼€æºæŒ‡ä»¤æ•°æ®é›† [COIG](https://huggingface.co/datasets/BAAI/COIG)ã€‚

## åè®®

æœ¬ä»“åº“çš„ä»£ç ä¾ç…§ [Apache-2.0](LICENSE) åè®®å¼€æºã€‚ChatGLM-6B æ¨¡å‹çš„ä½¿ç”¨è¯·éµå¾ª[æ¨¡å‹åè®®](https://github.com/THUDM/ChatGLM-6B/blob/main/MODEL_LICENSE)ã€‚

## å¼•ç”¨

å¦‚æœæ‚¨è§‰å¾—æ­¤é¡¹ç›®æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘ä»¥ä¸‹åˆ—æ ¼å¼å¼•ç”¨

```bibtex
@Misc{chatglm-efficient-tuning,
  title = {ChatGLM Efficient Tuning},
  author = {hiyouga},
  howpublished = {\url{https://github.com/hiyouga/ChatGLM-Efficient-Tuning}},
  year = {2023}
}
```

## å£°æ˜

æœ¬é¡¹ç›®å—ç›Šäº [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)ã€[ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning) å’Œ [yuanzhoulvpi2017/zero_nlp](https://github.com/yuanzhoulvpi2017/zero_nlp)ï¼Œæ„Ÿè°¢ä»¥ä¸Šè¯¸ä½ä½œè€…çš„ä»˜å‡ºã€‚

## Star History

![Star History Chart](https://api.star-history.com/svg?repos=hiyouga/ChatGLM-Efficient-Tuning&type=Date)
