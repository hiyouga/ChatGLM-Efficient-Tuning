# ChatGLM Efficient Tuning

![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/ChatGLM-Efficient-Tuning?style=social)
![GitHub Code License](https://img.shields.io/github/license/hiyouga/ChatGLM-Efficient-Tuning)
![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/ChatGLM-Efficient-Tuning)

åŸºäº ğŸ¤—[PEFT](https://github.com/huggingface/peft) çš„é«˜æ•ˆ ğŸ¤–[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) å¾®è°ƒã€‚

\[ [English](README.md) | ä¸­æ–‡ \]

## æ›´æ–°æ—¥å¿—

[23/04/12] ç°åœ¨æˆ‘ä»¬åŠ å…¥äº†æ–­ç‚¹è®­ç»ƒæ”¯æŒï¼è¯·å°è¯•ç»™å®š `--checkpoint_dir` å‚æ•°åŠ è½½æŒ‡å®šçš„æ¨¡å‹æ–­ç‚¹ã€‚

[23/04/11] ç°åœ¨æˆ‘ä»¬å®ç°äº†æ•°æ®é›†ç»„åˆè®­ç»ƒï¼è¯·å°è¯•ä½¿ç”¨ `--dataset dataset1,dataset2` å‚æ•°è¿›è¡Œç»„åˆè®­ç»ƒã€‚

## æ•°æ®é›†

ç›®å‰æˆ‘ä»¬å®ç°äº†é’ˆå¯¹ä»¥ä¸‹æ•°æ®é›†çš„æ”¯æŒï¼š

- [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)
- [Stanford Alpaca (Chinese)](https://github.com/ymcui/Chinese-LLaMA-Alpaca)
- [GPT-4 Generated Data](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)
- [BELLE 2M](https://huggingface.co/datasets/BelleGroup/train_2M_CN)
- [BELLE 1M](https://huggingface.co/datasets/BelleGroup/train_1M_CN)
- [BELLE 0.5M](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)
- [BELLE Dialogue 0.4M](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M)
- [BELLE School Math 0.25M](https://huggingface.co/datasets/BelleGroup/school_math_0.25M)
- [BELLE Multiturn Chat 0.8M](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M)
- [Guanaco Dataset](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)
- [Firefly 1.1M](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)

ä½¿ç”¨æ–¹æ³•è¯·å‚è€ƒ `config_data.py`ã€‚

## å¾®è°ƒæ–¹æ³•

ç›®å‰æˆ‘ä»¬å®ç°äº†é’ˆå¯¹ä»¥ä¸‹é«˜æ•ˆå¾®è°ƒæ–¹æ³•çš„æ”¯æŒï¼š

- [LoRA](https://arxiv.org/abs/2106.09685)
  - ä»…å¾®è°ƒä½ç§©é€‚åº”å™¨ã€‚
- [P-Tuning V2](https://github.com/THUDM/P-tuning-v2)
  - ä»…å¾®è°ƒå‰ç¼€ç¼–ç å™¨ã€‚
- [Freeze](https://arxiv.org/abs/2012.14913)
  - ä»…å¾®è°ƒåå‡ å±‚çš„å…¨è¿æ¥å±‚ã€‚

## è½¯ä»¶ä¾èµ–

- Python 3.10, PyTorch 2.0.0
- ğŸ¤—Transformers, Datasets, PEFTï¼ˆæœ€ä½éœ€è¦ 0.3.0.dev0ï¼‰
- protobuf, cpm_kernels, sentencepiece
- jieba, rouge_chinese, nltk

ä»¥åŠ **å¼ºè€Œæœ‰åŠ›çš„ GPU**ï¼

## å¦‚ä½•ä½¿ç”¨

### ç¯å¢ƒæ­å»ºï¼ˆå¯è·³è¿‡ï¼‰

```bash
git clone https://github.com/hiyouga/ChatGLM-Efficient-Tuning.git
conda create -n cet python=3.10
conda activate cet
pip install -r requirements.txt
```

### å¾®è°ƒè®­ç»ƒ

```bash
python finetune_chatglm.py \
    --do_train \
    --dataset alpaca_gpt4_zh \
    --finetuning_type lora \
    --output_dir output \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --max_train_samples 10000 \
    --learning_rate 5e-5 \
    --num_train_epochs 1.0 \
    --fp16
```

### æŒ‡æ ‡è¯„ä¼°ï¼ˆBLEUåˆ†æ•°å’Œæ±‰è¯­ROUGEåˆ†æ•°ï¼‰

```bash
python finetune_chatglm.py \
    --do_eval \
    --dataset alpaca_gpt4_zh \
    --checkpoint_dir output \
    --output_dir eval \
    --per_device_eval_batch_size 1 \
    --max_eval_samples 50 \
    --predict_with_generate
```

### æ•ˆæœæµ‹è¯•

```bash
python infer_chatglm.py --checkpoint_dir output
```

### ç¡¬ä»¶éœ€æ±‚

|     å¾®è°ƒæ–¹æ³•     |  æ‰¹å¤„ç†å¤§å°  | æ¨¡å¼ | GPUæ˜¾å­˜ | é€Ÿåº¦ |
| ---------------- | ---------- | ---- | ------ | ----- |
| LoRA (r=8)       |     8      | FP16 |  20GB  | 7ex/s |
| LoRA (r=8)       |     16     | FP16 |  26GB  | 8ex/s |
| P-Tuning (p=8)   |     8      | FP16 |  24GB  | 8ex/s |
| Freeze (l=2)     |     2      | FP16 |  32GB  | 4ex/s |

> rï¼šLoRA ç»´æ•°å¤§å°ï¼Œpï¼šå‰ç¼€è¯è¡¨å¤§å°ï¼Œlï¼šå¾®è°ƒå±‚æ•°ï¼Œex/sï¼šæ¯ç§’è®­ç»ƒçš„æ ·æœ¬æ•°
>
> ä¸Šè¿°ç»“æœå‡æ¥è‡ªäºå•ä¸ª Tesla V100 GPUã€‚

## å¾®è°ƒ ChatGLM çš„ä¾‹å­

### è®­ç»ƒç»“æœ

æˆ‘ä»¬ä½¿ç”¨æ•´ä¸ª `alpaca_gpt4_zh` æ•°æ®é›†å¾®è°ƒ ChatGLM æ¨¡å‹ï¼Œä½¿ç”¨ç§©ä¸º 8 çš„ LoRA æ–¹æ³•ï¼Œä½¿ç”¨é»˜è®¤è¶…å‚æ•°è¿›è¡Œå•è½®è®­ç»ƒã€‚ä¸‹å›¾ä¸ºè®­ç»ƒæŸå¤±å˜åŒ–æ›²çº¿ã€‚

![è®­ç»ƒæŸå¤±](assets/trainer_state.jpg)

### è¯„ä¼°ç»“æœ

æˆ‘ä»¬é€‰æ‹© `alpaca_gpt4_zh` æ•°æ®é›†ä¸­çš„å‰ä¸€ç™¾æ¡æ•°æ®æ¥è¯„ä¼°å¾®è°ƒåçš„ ChatGLM æ¨¡å‹ï¼Œå¹¶è®¡ç®— BLEU å’Œä¸­æ–‡ ROUGE åˆ†æ•°ã€‚ä¸‹è¡¨ä¸ºè¯„ä¼°ç»“æœã€‚

|   åˆ†æ•°  |  åŸç‰ˆæ¨¡å‹ |     LoRA (r=8)    |
| ------- | -------- | ----------------- |
| BLEU-4  |  15.75   | 17.01 (**+1.26**) |
| Rouge-1 |  34.51   | 36.77 (**+2.26**) |
| Rouge-2 |  15.11   | 16.83 (**+1.72**) |
| Rouge-l |  26.18   | 28.86 (**+2.68**) |

## å’Œç°æœ‰ç±»ä¼¼é¡¹ç›®çš„æ¯”è¾ƒ

- [THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B/tree/main/ptuning)
  - ChatGLM åŸºäº [P-Tuning v2](https://github.com/THUDM/P-tuning-v2) å¾®è°ƒçš„å®˜æ–¹å®ç°ï¼Œä½¿ç”¨äº† [ADGEN](https://aclanthology.org/D19-1321.pdf) æ•°æ®é›†ã€‚
  - æœ¬ä»“åº“çš„ä»£ç å®ç°ç»å¤§éƒ¨åˆ†å‚è€ƒè¯¥é¡¹ç›®ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å®ç°äº† [LoRA](https://arxiv.org/abs/2106.09685) å¾®è°ƒæ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬**åŠ¨æ€åœ°**å°†æ¯ä¸ªæ‰¹å¤„ç†æ•°æ®ä¸­çš„åºåˆ—è¿›è¡Œå¡«å……ï¼Œè€Œéå°†å…¶å¡«å……åˆ°æ¨¡å‹çš„æœ€å¤§é•¿åº¦ï¼Œæ­¤æ”¹è¿›å¯ä»¥åŠ é€Ÿæ¨¡å‹è®­ç»ƒã€‚
- [mymusise/ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning)
  - ChatGLM åŸºäº [LoRA](https://arxiv.org/abs/2106.09685) å¾®è°ƒçš„éå®˜æ–¹å®ç°ï¼Œä½¿ç”¨äº† [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) æ•°æ®é›†ã€‚
  - æˆ‘ä»¬å€Ÿé‰´äº†è¯¥é¡¹ç›®çš„ä¸€äº›æƒ³æ³•ã€‚æˆ‘ä»¬çš„è®­ç»ƒè„šæœ¬å°†æ•°æ®é¢„å¤„ç†éƒ¨åˆ†**é›†æˆ**è‡³è®­ç»ƒè„šæœ¬ä¸­ï¼Œä»¥é¿å…äº‹å…ˆç”Ÿæˆé¢„å¤„ç†åçš„æ•°æ®ã€‚
- [ssbuild/chatglm_finetuning](https://github.com/ssbuild/chatglm_finetuning)
  - ChatGLM åŸºäºå¤šç§å¾®è°ƒæ–¹æ³•çš„éå®˜æ–¹å®ç°ï¼Œä½¿ç”¨äº† [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) æ•°æ®é›†ã€‚
  - æˆ‘ä»¬çš„è®­ç»ƒè„šæœ¬**å…¨éƒ¨**åŸºäº [Huggingface transformers](https://github.com/huggingface/transformers) æ¡†æ¶å®ç°ï¼Œä¸ä¾èµ–äºé¢å¤–çš„ [deep_training](https://github.com/ssbuild/deep_training) æ¡†æ¶ã€‚
- [lich99/ChatGLM-finetune-LoRA](https://github.com/lich99/ChatGLM-finetune-LoRA)
  - ChatGLM åŸºäº [LoRA](https://arxiv.org/abs/2106.09685) å¾®è°ƒçš„éå®˜æ–¹å®ç°ï¼Œä½¿ç”¨äº† [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) æ•°æ®é›†ã€‚
  - æˆ‘ä»¬åˆ©ç”¨ [Huggingface PEFT](https://github.com/huggingface/peft) æ¡†æ¶æ¥å¼•å…¥æœ€å…ˆè¿›çš„å¾®è°ƒæ–¹æ³•ã€‚
- [liucongg/ChatGLM-Finetuning](https://github.com/liucongg/ChatGLM-Finetuning)
  - ChatGLM åŸºäºå‚æ•°å†»ç»“ã€LoRA å’Œ P-Tuning å¾®è°ƒçš„éå®˜æ–¹å®ç°ï¼Œä½¿ç”¨äº†æ±½è½¦å·¥ä¸šæ•°æ®é›†ã€‚
  - æˆ‘ä»¬æ—¨åœ¨å¼•å…¥æ›´å¤šæŒ‡ä»¤éµå¾ªæ•°æ®é›†ç”¨äºå¾®è°ƒ ChatGLM æ¨¡å‹ã€‚
- [yanqiangmiffy/InstructGLM](https://github.com/yanqiangmiffy/InstructGLM)
  - ChatGLM å¾®è°ƒçš„éå®˜æ–¹å®ç°ï¼Œæ—¨åœ¨æ¢ç´¢ ChatGLM åœ¨æŒ‡ä»¤éµå¾ªæ•°æ®é›†ä¸Šçš„æ½œåŠ›ã€‚
  - æˆ‘ä»¬å°†æ•°æ®é¢„å¤„ç†éƒ¨åˆ†é›†æˆåˆ°è®­ç»ƒè„šæœ¬ä¸­ã€‚

## TODO

- [ ] åŠ å…¥æ›´å¤š[ä¸­æ–‡æ•°æ®é›†](https://github.com/brightmart/nlp_chinese_corpus)ã€‚
  - [x] [BELLE](https://github.com/LianjiaTech/BELLE)
  - [ ] [pCLUE](https://github.com/CLUEbenchmark/pCLUE)
  - [ ] [CLUECorpus](https://github.com/CLUEbenchmark/CLUECorpus2020)
  - [x] [GuanacoDataset](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)
  - [x] [FireflyDataset](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)
- [ ] åŠ å…¥åŸºäº [ChatGPT](https://openai.com/blog/chatgpt) å’Œ [GPT-4](https://openai.com/research/gpt-4) äº§ç”Ÿçš„æ•°æ®é›†ã€‚
  - [ ] [Baize](https://github.com/project-baize/baize-chatbot)
  - [x] [GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)
- [x] å®ç°å‚æ•°å†»ç»“å’Œ P-Tuning å¾®è°ƒæ–¹æ³•ã€‚
- [ ] æ”¯æŒå¤šGPUè®­ç»ƒã€‚
- [x] åŠ å…¥æ¨¡å‹è¯„ä¼°è„šæœ¬ã€‚ï¼ˆä½†å®ƒå¯èƒ½å¾ˆæ…¢ï¼ï¼‰
- [x] æ–­ç‚¹åŠ è½½ã€‚
- [ ] ç»“åˆæ¨¡å‹ç¼–è¾‘æŠ€æœ¯ã€‚ï¼ˆä¾‹å¦‚ï¼š[MEND](https://arxiv.org/abs/2110.11309)ï¼‰
- [ ] ä½¿ç”¨ [DeepSpeed Chat](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat/chinese) ç»“åˆäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚

## åè®®

æœ¬ä»“åº“çš„ä»£ç ä¾ç…§ [Apache-2.0](LICENSE) åè®®å¼€æºã€‚

## å¼•ç”¨

å¦‚æœæ‚¨è§‰å¾—æ­¤é¡¹ç›®æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘ä»¥ä¸‹åˆ—æ ¼å¼å¼•ç”¨

```bibtex
@Misc{chatglm-efficient-tuning,
  title = {ChatGLM Efficient Tuning},
  author = {hiyouga},
  howpublished = {\url{https://github.com/hiyouga/ChatGLM-Efficient-Tuning}},
  year = {2023}
}
```


## å£°æ˜

æœ¬é¡¹ç›®å—ç›Šäº [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)ã€[ChatGLM-Tuning](https://github.com/THUDM/ChatGLM-6B) å’Œ [yuanzhoulvpi2017/zero_nlp](https://github.com/yuanzhoulvpi2017/zero_nlp)ï¼Œæ„Ÿè°¢ä½œè€…çš„ä»˜å‡ºã€‚
