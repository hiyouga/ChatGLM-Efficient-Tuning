# åŸºäºæœ¬æ¡†æ¶çš„å¿«é€Ÿå¾®è°ƒchatGLMçš„ç®€æ˜“æ­¥éª¤

![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/ChatGLM-Efficient-Tuning?style=social)
![GitHub Code License](https://img.shields.io/github/license/hiyouga/ChatGLM-Efficient-Tuning)
![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/ChatGLM-Efficient-Tuning)
![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)

ğŸ‘‹ **åŠ å…¥æˆ‘ä»¬çš„[å¾®ä¿¡ç¾¤](assets/wechat.jpg)ã€‚**

\[ [English](README_fine_tune_EN.md) | ä¸­æ–‡ \]

## æ›´æ–°æ—¥å¿—

[23/04/19] æˆ‘ä»¬è¡¥å……äº†åŸºäºæœ¬æ¡†æ¶çš„å¾®è°ƒç®€æ˜“æ­¥éª¤ä¾›å‚è€ƒã€‚

# å¾®è°ƒæ­¥éª¤

## **1.ä¾èµ–ç¯å¢ƒé¢„å®‰è£…ï¼š**

å»ºè®®ä½¿ç”¨condaæˆ–è€…pipenvç­‰è™šæ‹Ÿç¯å¢ƒå®‰è£…ä¾èµ–ï¼Œèƒ½å°½æœ€å¤§é™åº¦ä¸å½±å“ç³»ç»Ÿé…ç½®å¹¶ç®¡ç†é¡¹ç›®ä¾èµ–çš„ç¯å¢ƒåŒ…ï¼Œä»¥ä¸‹ä»¥condaä¸ºä¾‹å­

```bash
git clone https://github.com/hiyouga/ChatGLM-Efficient-Tuning.git
conda create -n cet python=3.10
conda activate cet
cd ChatGLM-Efficient-Tuning
pip install -r requirements.txt
```

**æœ¬é¡¹ç›®é»˜è®¤ä¼šè”ç½‘è·å–æœ€æ–°çš„chatGLMæ¨¡å‹ç›¸å…³æ–‡ä»¶ï¼Œå› ä¸ºæ¨¡å‹ç‰ˆæœ¬å¯èƒ½å½±å“ä»£ç æ•ˆæœç”šè‡³æŠ¥é”™ï¼Œå»ºè®®ä½¿ç”¨å®˜æ–¹æœ€æ–°çš„æ¨¡å‹ç‰ˆæœ¬æ–‡ä»¶ï¼Œå¦‚è‡ªèº«ç½‘ç»œä¸ä½³å¶å°”ä¼šå‡ºç°connection time outç­‰ç°è±¡ï¼ˆæ¯•ç«Ÿhuggingfaceå¢™å†…ä¸èƒ½ä¿è¯ç½‘é€Ÿï¼‰ï¼Œè¯·å°è¯•å…ˆç¦»çº¿ä¸‹è½½å®˜æ–¹æ¨¡å‹ä¿å­˜åˆ°æœ¬åœ°**

ç„¶åä¿®æ”¹config.pyæ–‡ä»¶çš„CHATGLM_REPO_NAMEè·¯å¾„åœ°å€ä¸ºæœ¬åœ°ç¦»çº¿ä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶åœ°å€å¦‚å›¾æ‰€ç¤ºï¼š

![image-20230419201703839](../assets/images/image-20230419201703839.png)

## 2.**æ•°æ®é›†å‡†å¤‡ï¼š**

æœ¬é¡¹ç›®æ”¯æŒå¤šç§ä¸»æµçš„æ•°æ®é›†ç§ç±»å¸®åŠ©ç”¨æˆ·èŠ‚çœæ‰¾å¯»æ•°æ®é›†æµ‹è¯•çš„æ—¶é—´ï¼ˆæ•°æ®è§„æ¨¡åªæ˜¯åˆæ­¥ç»Ÿè®¡ï¼Œæ•°æ®é›†è·Ÿéšæºä»“åº“å¯èƒ½æ›´æ–°ï¼‰ï¼š

| æ•°æ®é›†åç§°                                                   | è§„æ¨¡ï¼ˆä¼°ç®—ï¼‰ | æè¿°                                                         |
| ------------------------------------------------------------ | ------------ | ------------------------------------------------------------ |
| [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) | 52k          | æ–¯å¦ç¦å¤§å­¦å¼€æºçš„alpacaæ•°æ®é›†ï¼Œè®­ç»ƒäº†alpacaè¿™ç±»æ—©æœŸåŸºäºLLaMAçš„æ¨¡å‹ |
| [Stanford Alpaca (Chinese)](https://github.com/ymcui/Chinese-LLaMA-Alpaca) | 51k          | é’ˆå¯¹åŸç‰ˆLLaMAæ¨¡å‹æ‰©å……äº†ä¸­æ–‡è¯è¡¨                              |
| [GPT-4 Generated Data](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM) | 100k+        | åŸºäºGPT4.0çš„self-instructionæ•°æ®é›†                           |
| [BELLE 2M](https://huggingface.co/datasets/BelleGroup/train_2M_CN) | 2M           | åŒ…å«çº¦200ä¸‡æ¡ç”±[BELLE](https://github.com/LianjiaTech/BELLE)é¡¹ç›®ç”Ÿæˆçš„ä¸­æ–‡æŒ‡ä»¤æ•°æ® |
| [BELLE 1M](https://huggingface.co/datasets/BelleGroup/train_1M_CN) | 1M           | åŒ…å«çº¦100ä¸‡æ¡ç”±[BELLE](https://github.com/LianjiaTech/BELLE)é¡¹ç›®ç”Ÿæˆçš„ä¸­æ–‡æŒ‡ä»¤æ•°æ®ã€‚ |
| [BELLE 0.5M](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN) | 0.5M         | åŒ…å«çº¦50ä¸‡æ¡ç”±[BELLE](https://github.com/LianjiaTech/BELLE)é¡¹ç›®ç”Ÿæˆçš„ä¸­æ–‡æŒ‡ä»¤æ•°æ®ã€‚ |
| [BELLE Dialogue 0.4M](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M) | 0.4M         | åŒ…å«çº¦40ä¸‡æ¡ç”±[BELLE](https://github.com/LianjiaTech/BELLE)é¡¹ç›®ç”Ÿæˆçš„ä¸ªæ€§åŒ–è§’è‰²å¯¹è¯æ•°æ®ï¼ŒåŒ…å«è§’è‰²ä»‹ç»ã€‚æ³¨æ„**æ­¤æ•°æ®é›†æ˜¯chatGPTäº§ç”Ÿçš„æ•°æ®é›†ï¼Œä¸ä¿è¯æ•°æ®å‡†ç¡®æ€§**ã€‚æ‰€æœ‰ç±»GPTæ¨¡å‹äº§ç”Ÿçš„self-instructionéƒ½ä¸èƒ½ä¿è¯å‡†ç¡®ç‡ã€‚ |
| [BELLE School Math 0.25M](https://huggingface.co/datasets/BelleGroup/school_math_0.25M) | 0.25M        | åŒ…å«çº¦25ä¸‡æ¡ç”±[BELLE](https://github.com/LianjiaTech/BELLE)é¡¹ç›®ç”Ÿæˆçš„ä¸­æ–‡æ•°å­¦é¢˜æ•°æ®ï¼ŒåŒ…å«è§£é¢˜è¿‡ç¨‹ |
| [BELLE Multiturn Chat 0.8M](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M) | 0.8M         | åŒ…å«çº¦80ä¸‡æ¡ç”±[BELLE](https://github.com/LianjiaTech/BELLE)é¡¹ç›®ç”Ÿæˆçš„*ç”¨æˆ·*ä¸*åŠ©æ‰‹*çš„å¤šè½®å¯¹è¯ |
| [Guanaco Dataset](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset) | 100k+        | åŒ…å«æ—¥æ–‡ã€ç®€ç¹ä½“ä¸­æ–‡ã€è‹±æ–‡ç­‰å¤šç±»æ•°æ®ï¼Œæ•°æ®é›†åŸç”¨äºGuanacoæ¨¡å‹è®­ç»ƒã€‚ |
| [Firefly 1.1M](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M) | 1.1M         | ä¸­æ–‡å¯¹è¯å¤§æ¨¡å‹fireflyï¼ˆæµè¤ï¼‰çš„ä¸­æ–‡æ•°æ®é›†ï¼Œå¯¹äºæ¯ä¸ªä»»åŠ¡ï¼Œç”±äººå·¥ä¹¦å†™è‹¥å¹²ç§æŒ‡ä»¤æ¨¡æ¿ |

ç”¨æˆ·å¯ä»¥ä¸‹è½½ä»¥ä¸Šæ•°æ®é›†ä¿å­˜åˆ°dataç›®å½•ä¸‹ï¼Œç„¶åä¿®æ”¹src/config.pyæ–‡ä»¶æ–°å¢æ•°æ®é›†çš„ä¿¡æ¯è¿›è¡Œæ¨¡å‹å¾®è°ƒ

æ•°æ®é›†æ ¼å¼ç±»ä¼¼å¦‚ä¸‹ï¼š

![image-20230419220019393](/Users/kidd/Documents/Repository/images/image-20230419220019393-1918217-1918809.png)

å¿…å¡«é¡¹ï¼š**instructionã€output.**

é€‰å¡«é¡¹ï¼šhistoryï¼Œå¦‚æ•°æ®åœºæ™¯æ¶‰åŠå¤šè½®ä¼šè¯ä¸Šä¸‹æ–‡è”ç³»çš„ï¼Œå»ºè®®é‡‡ç”¨é«˜è´¨é‡historyå¤šè½®ä¼šè¯æ•°æ®é›†ï¼Œhistoryé»˜è®¤å¯ä»¥ä¸ºç©ºï¼Œä¼ å…¥ç©ºæ•°ç»„[]å³å¯ã€‚

**Tipsï¼š**

â€‹		 2.1.å¾ˆå¤šç”¨æˆ·ä¸Šæ‰‹å¾®è°ƒçš„ç¬¬ä¸€ä¸ªå°å®éªŒéƒ½æ˜¯ä»è®­ç»ƒæ¨¡å‹æ”¹å˜è‡ªæˆ‘èº«ä»½çš„å›ç­”å¼€å§‹ï¼Œä¾‹å¦‚é—®æ¨¡å‹èº«ä»½æ˜¯è°è¿™ç±»åœºæ™¯ï¼Œéœ€è¦ç‰¹åˆ«æ³¨æ„ï¼Œè¿™ç§é’ˆå¯¹å•ç±»é—®é¢˜å›ç­”åˆè¦æ±‚ç›¸å¯¹è¾ƒé«˜çš„åœºåˆï¼Œéœ€è¦æ•°æ®é›†é‡Œé¢ä¿æŒä¸€å®šæ¯”ä¾‹çš„å…³äºè‡ªèº«é—®é¢˜çš„å¯¹è¯ï¼Œä¸€èˆ¬æˆ‘ä»¬å»ºè®®ä¸ä½äºæ•°æ®é›†çš„5%ï¼Œä¸”é—®ç­”å†…å®¹å°½é‡ä¿æŒä¸€å®šå·®å¼‚ï¼Œè¿™æ ·è®©æ¨¡å‹èƒ½å……åˆ†å­¦ä¹ åˆ°è‡ªæˆ‘è®¤çŸ¥çš„å›ç­”ã€‚

â€‹		2.2.å¦‚æœç”¨æˆ·è‡ªå®šä¹‰æ•°æ®é›†ï¼Œè¯·å°½é‡æŠŠæ•°æ®é›†è½¬æ¢ä¸ºjsonæ–‡ä»¶ï¼Œæ–¹ä¾¿åé¢å¯¹é½æ–‡æœ¬æ“ä½œã€‚

## 3.æ‰§è¡Œå¾®è°ƒç¨‹åº

åˆ‡æ¢åˆ°**src/finetune.py**ç›®å½•ä¸‹ï¼Œæ­¤æ–‡ä»¶æ˜¯å¾®è°ƒä¸»ç¨‹åºæ–‡ä»¶ï¼Œåˆ°æ­¤ç›®å½•ä¸‹æ‰§è¡Œpythonå‘½ä»¤

â€‹	**æœ¬é¡¹ç›®æ”¯æŒå•å°å•GPUä¸å•å°å¤šGPUå¾®è°ƒ**

**å•GPU**è°ƒç”¨å‘½ä»¤å‚æ•°å¦‚ä¸‹ï¼š

```bash
CUDA_VISIBLE_DEVICES=1 python finetune.py  \
		--do_train \
		--dataset instruct_ch\     
    --finetuning_type lora \
    --output_dir output/0418-02 \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate 1e-4 \
    --num_train_epochs 1.0 \
    --fp16
		#æ³¨æ„python finetune_chatglm.pyæ‰§è¡Œçš„æ—¶å€™ï¼Œéœ€è¦æŒ‡å®šè‡ªå·±ç¯å¢ƒé‡Œé¢ pythonè·¯å¾„ï¼Œå½“ç„¶å¦‚æœæ˜¯æ ¹æ®ä¸Šé¢ä½¿ç”¨condaä¹‹ç±»çš„è™šæ‹Ÿç¯å¢ƒå°±ä¸			å¿…åœ¨æ„è¿™ä¸ªé—®é¢˜äº†ã€‚
    #CUDA_VISIBLE_DEVICESå‚æ•°å¯ä»¥é»˜è®¤ä¸ä¼ ï¼Œä»¥ä¸‹ä¾‹å­ä¼ äº†1æŒ‡å®šåºå·1çš„æœ¬æœºGPUè¿è¡Œç¨‹åºï¼Œä¹Ÿå¯ä»¥æ˜¯0æˆ–è€…å…¶ä»–åºå·çš„GPU
    #ä¸»è¦å‚æ•°è§£é‡Šï¼š
    #							--dataset æ˜¯æ•°æ®é›†åç§°ï¼Œæ­¤å¤„æ•°æ®é›†åç§°è¦ä¸config.pyé‡Œé¢è‡ªå®šä¹‰çš„æ•°æ®é›†åç§°ç›¸åŒ
    #                   æ”¯æŒå¤šä¸ªæ•°æ®é›†åŒæ—¶å¯¼å…¥ï¼Œæ ¼å¼ç±»ä¼¼ --dataset instruct_ch,instruct_ch2,instruct_ch3......
    #							--finetuning_type è¡¨ç¤ºå¾®è°ƒæ–¹æ³•ï¼Œç›®å‰æœ¬é¡¹ç›®å•GPUæ”¯æŒLoRAã€freezeã€ptuningä¸‰ç§æ–¹æ³•ï¼Œå¯¹åº”åç§°ä¿®æ”¹å°±è¡Œ
    #							--output_dirä¸ºæ¨¡å‹è¾“å‡ºç»“æœç›®å½•ï¼Œæ–­ç‚¹ç»­è®­ç»ƒä¹Ÿæ˜¯éœ€è¦æŒ‡å®šcheckpointåœ°å€ä»è¿™è¾¹å¼€å§‹
    #							--per_device_train_batch_size æ¯æ‰¹æ¬¡æ ·æœ¬æ•°é‡ï¼Œæ˜¾å­˜è¶Šå°å»ºè®®batchsizeè°ƒä½ã€‚
    #							--lr_scheduler_type æ”¯æŒlinerè·Ÿcosineä¸¤ç§æ–¹å¼ï¼Œç”¨æˆ·è‡ªè¡Œé€‰æ‹©è‡ªå·±æ‰€å±åœºæ™¯æµ‹è¯•å“ªç§åˆé€‚ã€‚
    #							--logging_steps æ—¥å¿—è®°å½•çš„stepé—´éš”æ•°ï¼Œå¦‚æœä¸æƒ³debugï¼Œå»ºè®®å¢å¤§è®°å½•æ­¥æ•°ï¼Œå‡å°‘IOè¯»å†™
    #							--learning_rate å­¦ä¹ æ›²ç‡ï¼Œæ ¹æ®è‡ªèº«éœ€æ±‚å¯ä»¥å¼¹æ€§è°ƒèŠ‚ã€‚
    #							--num_train_epochs epochçš„æ•°ç›®ï¼Œå¦‚æœæ¨¡å‹æ¬ æ‹Ÿåˆæ•°æ®é›†è¾ƒå¤§å¯ä»¥é€‚å½“è°ƒé«˜ï¼Œç”¨æˆ·åº”æ ¹æ®è®­ç»ƒç»“æœæ˜¯è¿‡æ‹Ÿåˆæˆ–è€…æ¬ æ‹Ÿåˆç»“										æœæ¥è°ƒæ•´ã€‚
    #							--fp16 è¡¨ç¤ºäº†é‡åŒ–ç²¾åº¦ï¼Œæ”¯æŒ16,8,4ä¸‰ç§ç²¾åº¦ï¼Œæ ¹æ®æœ¬æœºçš„æ€§èƒ½é…ç½®ï¼Œå¦‚æœå‡ºç°cuda out of memoryä¹‹ç±»é”™è¯¯çš„å»ºè®®ä¸										ä½¿ç”¨16ï¼Œè°ƒä½åˆ°8æˆ–è€…4é€æ­¥å°è¯•å°½é‡ç”¨å®Œè‡ªå·±çš„æœºå™¨æ€§èƒ½
    
   
```

ä»¥ä¸Šä¼ å‚å¦‚æœè§‰å¾—æ¯æ¬¡éƒ½è¦å‘½ä»¤è¡Œè¾“å…¥å¤ªå¤šå¤ªéº»çƒ¦ï¼Œè¿˜å¯ä»¥é€šè¿‡ä¿®æ”¹bashè„šæœ¬**examples/finetune.sh**æ–‡ä»¶ï¼Œé€šè¿‡è„šæœ¬æ–‡ä»¶çš„ä¿®æ”¹åªéœ€è¦æ‰§è¡Œsh ./finetune.shäº¦å¯è¿›è¡Œè°ƒå‚ã€‚

**å¤šGPUåŠ é€Ÿå¾®è°ƒ**

```bash
accelerate launch finetune.py  \
		--do_train \
		--dataset instruct_ch\     
    --finetuning_type freeze \
    --output_dir output/0418-02 \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate 1e-4 \
    --num_train_epochs 1.0 \
    --fp16
    #å‚æ•°å«ä¹‰åŒä¸Šå•GPUï¼Œæ³¨æ„ç›®å‰å¤šGPUæ¨¡å¼æš‚ä¸æ”¯æŒLoRAæ–¹æ³•ï¼Œæ¨èå…ˆé‡‡ç”¨freezeæ–¹æ³•ï¼Œä¸€æ ·æé«˜çº¦ä¸€å€çš„è®­ç»ƒæ•ˆç‡
```



## **4.ä½¿ç”¨æ–°å¾®è°ƒåçš„æ¨¡å‹**

```bash
CUDA_VISIBLE_DEVICES=0 python infer_chatglm.py --checkpoint_dir chatglm_efficient_tuning/output/0418-02/

å¯ä»¥æŒ‡å®šä½¿ç”¨å“ªå—GPUè¿è¡Œæ¨ç†æ–‡ä»¶ï¼Œcheckpointç›®å½•è¦æ”¹æˆä¸Šé¢å¾®è°ƒåä¿å­˜çš„outputä¸‹ç›®å½•åœ°å€

```

æˆåŠŸè¿è¡Œåçš„ç•Œé¢å¦‚ä¸‹æ‰€ç¤ºï¼š

![image-20230419214456931](../assets/images/image-20230419214456931-1918651.png)

æ³¨æ„ä¼šè¯å¤šè½®åå ç”¨è®¡ç®—èµ„æºä¼šå¢åŠ ï¼Œæ ¹æ®è‡ªèº«åœºæ™¯éœ€æ±‚ä¸æœºå™¨é…ç½®ä½¿ç”¨ã€‚

éƒ¨ç½²å¯è§†åŒ–ç•Œé¢ï¼š

å¦‚æœç”¨æˆ·æƒ³å¤ç”¨chatGLMå®˜æ–¹çš„Gradioå‰ç«¯ç•Œé¢è¿è¡Œæœ¬æ¡†æ¶å¾®è°ƒåçš„ä»£ç ä¹Ÿè¡Œï¼š

```python
from utils	import utils, arguments #æ ¹æ®è‡ªå·±å®‰è£…çš„æœ¬æ¡†æ¶ç›®å½•é€‚å½“è°ƒæ•´fromçš„utilsæ–‡ä»¶å¤¹ä½ç½®å¯¼å…¥utilsè·Ÿarguments
    model_path="chatglm_efficient_tuning/output/0417/"
    model_args=ModelArguments(checkpoint_dir=model_path)
    model, tokenizer = load_pretrained(model_args)
    model = model.half().cuda()
```

python webdemo.pyè¿è¡Œåæ—¢èƒ½å¤ç”¨å®˜æ–¹å¯è§†åŒ–ç•Œé¢åˆèƒ½åŸºäºæœ¬æ¡†æ¶å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæµ‹è¯•ã€‚









## åè®®

æœ¬ä»“åº“çš„ä»£ç ä¾ç…§ [Apache-2.0](LICENSE) åè®®å¼€æºã€‚

## å¼•ç”¨

å¦‚æœæ‚¨è§‰å¾—æ­¤é¡¹ç›®æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘ä»¥ä¸‹åˆ—æ ¼å¼å¼•ç”¨

```bibtex
@Misc{chatglm-efficient-tuning,
  title = {ChatGLM Efficient Tuning},
  author = {hiyouga},
  howpublished = {\url{https://github.com/hiyouga/ChatGLM-Efficient-Tuning}},
  year = {2023}
}
```

## å£°æ˜

æœ¬é¡¹ç›®å—ç›Šäº [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)ã€[ChatGLM-Tuning](https://github.com/mymusise/ChatGLM-Tuning) å’Œ [yuanzhoulvpi2017/zero_nlp](https://github.com/yuanzhoulvpi2017/zero_nlp)ï¼Œæ„Ÿè°¢ä½œè€…çš„ä»˜å‡ºã€‚
